{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HsGqubiDkQnd"
      },
      "outputs": [],
      "source": [
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7iS40R9okStg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\khale\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gym\\envs\\registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment FrozenLakeNotSlippery-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        }
      ],
      "source": [
        "gym.envs.register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        "    max_episode_steps=100,\n",
        "    reward_threshold=0.74\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FVX1AjRWkueO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 4, 0.0, False)],\n",
              "  2: [(1.0, 1, 0.0, False)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 1: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 5, 0.0, True)],\n",
              "  2: [(1.0, 2, 0.0, False)],\n",
              "  3: [(1.0, 1, 0.0, False)]},\n",
              " 2: {0: [(1.0, 1, 0.0, False)],\n",
              "  1: [(1.0, 6, 0.0, False)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 3: {0: [(1.0, 2, 0.0, False)],\n",
              "  1: [(1.0, 7, 0.0, True)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 3, 0.0, False)]},\n",
              " 4: {0: [(1.0, 4, 0.0, False)],\n",
              "  1: [(1.0, 8, 0.0, False)],\n",
              "  2: [(1.0, 5, 0.0, True)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(1.0, 5, 0.0, True)],\n",
              "  1: [(1.0, 10, 0.0, False)],\n",
              "  2: [(1.0, 7, 0.0, True)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 12, 0.0, True)],\n",
              "  2: [(1.0, 9, 0.0, False)],\n",
              "  3: [(1.0, 4, 0.0, False)]},\n",
              " 9: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 10, 0.0, False)],\n",
              "  3: [(1.0, 5, 0.0, True)]},\n",
              " 10: {0: [(1.0, 9, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 11, 0.0, True)],\n",
              "  3: [(1.0, 6, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(1.0, 12, 0.0, True)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 14, 0.0, False)],\n",
              "  3: [(1.0, 9, 0.0, False)]},\n",
              " 14: {0: [(1.0, 13, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 15, 1.0, True)],\n",
              "  3: [(1.0, 10, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the gridworld-like environment\n",
        "env=gym.make('FrozenLakeNotSlippery-v0')\n",
        "# Let's look at the model of the environment (i.e., P):\n",
        "env.env.P\n",
        "# Question: what is the data in this structure saying? Relate this to the course\n",
        "# presentation of P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_6Sp3Hltz2D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Gyn_w3ulkyZI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(16)\n"
          ]
        }
      ],
      "source": [
        "# Now let's investigate the observation space (i.e., S using our nomenclature),\n",
        "# and confirm we see it is a discrete space with 16 locations\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zND5ArI8k_qQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        }
      ],
      "source": [
        "stateSpaceSize = env.observation_space.n\n",
        "print(stateSpaceSize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R_tp9YzRljnj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "# Now let's investigate the action space (i.e., A) for the agent->environment\n",
        "# channel\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fFGNZNowluz2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample from S: 1  ...  sample from A: 3\n",
            "sample from S: 0  ...  sample from A: 2\n",
            "sample from S: 5  ...  sample from A: 1\n",
            "sample from S: 14  ...  sample from A: 3\n",
            "sample from S: 13  ...  sample from A: 1\n",
            "sample from S: 6  ...  sample from A: 2\n",
            "sample from S: 0  ...  sample from A: 2\n",
            "sample from S: 5  ...  sample from A: 0\n",
            "sample from S: 4  ...  sample from A: 2\n"
          ]
        }
      ],
      "source": [
        "# The gym environment has ...sample() functions that allow us to sample\n",
        "# from the above spaces:\n",
        "for g in range(1,10,1):\n",
        "  print(\"sample from S:\",env.observation_space.sample(),\" ... \",\"sample from A:\",env.action_space.sample())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nOQL5JxsmcEd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\khale\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLakeNotSlippery-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "# The enviroment also provides a helper to render (visualize) the environment\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KLV6e43mmwx1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the action as an integer from 0 to 4  (or exit): \n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32md:\\Documents\\APS1081_Quantum_Machine_Learning\\A1_DynProg.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/APS1081_Quantum_Machine_Learning/A1_DynProg.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/APS1081_Quantum_Machine_Learning/A1_DynProg.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m action\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(userInput)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/APS1081_Quantum_Machine_Learning/A1_DynProg.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m (observation, reward, compute, probability) \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/APS1081_Quantum_Machine_Learning/A1_DynProg.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m--> The result of taking action\u001b[39m\u001b[39m\"\u001b[39m,action,\u001b[39m\"\u001b[39m\u001b[39mis:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/APS1081_Quantum_Machine_Learning/A1_DynProg.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m     S=\u001b[39m\u001b[39m\"\u001b[39m,observation)\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ],
      "source": [
        "# We can act as the agent, by selecting actions and stepping the environment\n",
        "# through time to see its responses to our actions\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "while not(exitCommand):\n",
        "  env.render()\n",
        "  print(\"Enter the action as an integer from 0 to\",env.action_space.n,\" (or exit): \")\n",
        "  userInput=input()\n",
        "  if userInput==\"exit\":\n",
        "    break\n",
        "  action=int(userInput)\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1tBpeiuRnyih"
      },
      "outputs": [],
      "source": [
        "# Question: draw a table indicating the correspondence between the action\n",
        "# you input (a number) and the logic action performed.\n",
        "# Question: draw a table that illustrates what the symbols on the render image\n",
        "# mean?\n",
        "# Question: Explain what the objective of the agent is in this environment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWI3h6s7qqdq"
      },
      "outputs": [],
      "source": [
        "# Practical: Code up an AI that will employ random action selection in order\n",
        "# to drive the agent. Test this random action selection agent with the\n",
        "# above environment (i.e., code up a loop as I did above, but instead\n",
        "# of taking input from a human user, take it from the AI you coded)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmRwGwPoqw0F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQLKuubzrl4L"
      },
      "outputs": [],
      "source": [
        "# Now towards dynamic programming. Note that env.env.P has the model\n",
        "# of the environment.\n",
        "#\n",
        "# Question: How would you represent the agent's policy function and value function?\n",
        "# Practical: revise the above AI solver to use a policy function in which you\n",
        "# code the random action selections in the policy function. Test this.\n",
        "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\n",
        "# either the inplace or ping-pong buffer (as described in the lecture). Now\n",
        "# randomly initialize your policy function, and compute its value function.\n",
        "# Report your results: policy and value function. Ensure your prediction\n",
        "# algo reports how many iterations it took.\n",
        "#\n",
        "# (Optional): Repeat the above for q.\n",
        "#\n",
        "# Policy Improvement:\n",
        "# Question: How would you use P and your value function to improve an arbitrary\n",
        "# policy, pi, per Chapter 4?\n",
        "# Practical: Code the policy iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, and ensure\n",
        "# it reports the number of iterations for each step: (a) overall policy\n",
        "# iteration steps and (b) evaluation steps.\n",
        "# Practical: Code the value iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, reporting\n",
        "# the iteration counts.\n",
        "# Comment on the difference between the iterations required for policy vs\n",
        "# value iteration.\n",
        "#\n",
        "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\n",
        "# env = gym.make(\"FrozenLake-v0\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "A1_FrozenLake.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
